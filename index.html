<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<style>
    .image-container {
      display: flex; /* 设置为flex布局 */
      padding-bottom: 10px;
    }

    .image-container img {
      width: 50%; /* 图片占据父容器的50%宽度 */
      height: auto; /* 保持宽高比例 */
      padding-right: 10px;
    }
    .image-container2 img {

      padding-right: 1px;
    }
    
</style>

**Computer Graphic - Final Report**

Student Name: Yuhang Qiu, Zehang Qiu

Legi Number: 22-739-783(YQ), 22-739-809(ZQ)




# Motivation

<img src="./images/MotivationalImage.png" alt="Motivational image" class="img-responsive">

The motivational image we have chosen is a creation generated by Stable Diffusion+ControlNet.
This image, crafted by the Twitter user MrUgleh in September, features a distinct checkerboard
pattern. Despite the initial appearance of weirdness, upon closer examination, each square 
reveals a sense of order and coincidence.


<br>
<font size=5> Reporter: Yuhang Qiu </font>

# Implementation
Feature List <br>
<ul>
    <li>5  pts - Modeling mesh </li>
    <li>5  pts - Object instancing</li>
    <li>5  pts - Addictional emitter: distant light</li>
    <li>5  pts - Intel denoising</li>
    <li>10 pts - MipMaping</li>
    <li>15 pts - Environment map emitter</li>
    <li>15 pts - Realistic camera model</li>
</ul>


## 5 pts  Modeling Mesh

To achieve the sense of order and coincidence in a picture, we need to select proper objects and place them in the correct direction and position. 
And in some of the cases, we need to modified a little bit on the objects to generate a perfect division in vision.
<br><br>
I have been working in blender by doing translating, rotating, scaling, and model modification on models, camera, and the light source in the scene, 
to make the whole picture to achieve the effect that we expected under the effect of light and shadow.
<br><br>
I also build a grid framework to help me adjusting object in the camera view.
<div class="image-container">
    <img src="images/model_overview.png" alt="Overview of the scene">
    <img src="images/model_camera.png" alt="Cameraview of the scene">
</div>
<div class="image-container">
    <img src="images/model_modeling.png" alt="Modify shape">
    <img src="images/model_mirror.png" alt="Creat window with mirror and frame">
</div>
Then with the help of Blender Add-ons Nori-exporter, I can export the modified meshes and a raw XML describing the well designed scene.
But the textures are excluded so we need to attach the texture on object manually.


## 5 pts  Object Instancing
**Related source file: **
<ul>
    <li>include/nori/instanct.h    </li>
    <li>include/nori/reference.h</li>
    <li>include/nori/shape.h</li>
    <li>src/reference.cpp</li>
    <li>src/instance.cpp</li>
    <li>src/scene.cpp</li>
</ul>

According to the Issue#107 nori-base-23 I decided to create two new subclass of shape, 
representing reference and instance thus we can directly add them into xml. 
And the instance object only carries transformation from local(reference) to the world, which can be set within xml.
<br><br>
All the intersections to be happen with this type of shape as well as the intersecections recording is processd is in a local(reference) coordinate.
<br><br>

**reference.h & reference.cpp**
<br>
A simple copy of shape, to store reference object and local infomation.

**instance.h & instance.cpp**
<br>
The class representing instance. The shape-methods are overrided, to map the ray into local coordinate,
and setHitInfo in local coordinat. After that turn the HitInfo back to the world coordinat.
 
**scene.cpp & shape.h**
<br>
Modified and added some functions to support the instancing system.
<br>

<div class="twentytwenty-container">
    <img src="images/Instance.png" alt="Instancing" class="img-responsive">
    <img src="images/noInstance.png" alt="Commented Instances" class="img-responsive">
</div>



## 5 pts Distant Light & 15 pts Environment Map Emitter
**Related source file: **
<ul>
    <li>include/nori/emitter.h    </li>
    <li>src/distantlight.cpp</li>
    <li>src/envlight.pp</li>
    <li>src/path_mis.cpp</li>
</ul>

According to the book pbrtv3, these two emitters are infinite emitters. 
They are emitting light from a super far distant as identify. And they don't have solid objects exist in the scene. 
So when calculating their light contribution in intergrator, we need to go in another way, because it is impossible for the path tracing light to hit these emitter without entities.
And these emitters need to provid their special eval method.
<br><br>
**emitter.h**
<br>
Add Le() method to be overrided in infinite emitters subclass.

**distantlight.cpp**
<br>
All the light can be seem as parallel since they come from far far away(like sunlight). So the init parameter would be irradience and direction.
<br>when being sampled, return m_radiance and set light m_directoin directly. <br>
And the eval() function should return the intergration of the light irradience witin the scene bounding area.<br>
Like the implementation of pointlight, pdf() return 1.0 .<br>
Distant light contribution can be collect as a random emitter in getRandomEmitter() in emitter sampling part, and simply contribute its radience by calling sample().

<br>
<div class="twentytwenty-container">
    <img src="images/distant_nori.png" alt="Nori" class="img-responsive">
    <img src="images/distant_mitsuba.png" alt="Mitsuba3" class="img-responsive">
</div>

**envlight.cpp**
<br>
A kind of light is an infinitely far-away area light source that surrounds the entire scene. It support user to attach environment texture on it. <br>
The way it sampling light is performing a pipeline from sampling point in uv map into the light direction that hit the scene.<br>
To do sampling in uvmap we need to compute the sample weight of each pixel on map, so the first work is to precompute the PDF map of the uvMap
by the radience that the pixel carries. Then we can perform weighted sampling on the uvMap, 
to achieve the effect of the brighter a area is, the more possible the area would emitting light.(like the light source in the environment)  <br>
Then perform the transformation from uv Coordinate, to sphere Coordinate then to the query direction in world coordinate.
And return the interpolated radience(color) in the vuMap with corresponding pdf.<br>

<div class="twentytwenty-container">
    <img src="images/evn_nori.png" alt="Nori" class="img-responsive">
    <img src="images/env_mitsuba.png" alt="Mitsuba3" class="img-responsive">
</div>

**path_mis.cpp**
<br>
Because the light would never hit these two emitters, so the old intergrator will never have mats term contribution, and the part that doesn't hit any object would remain black.<br>
So I add two way to fit these emitters. One is when the tracing ray doesn't hit any object and escape the scene, it can be seem that it hit the covering environment emitter shpere. 
We need to query the color with the escaping ray Rec in the new Emitter method Le(Rec)<br>
And when doing Emitter sampling, these emitter should be seem as normal emitters to being smapled, so we just need to make sure the sample() result is correct.<br>


<div class="twentytwenty-container">
    <img src="images/multilight_nori.png" alt="Multi-light Nori" class="img-responsive">
    <img src="images/multilight_mitsuba.png" alt="Multi-light Mitsuba3" class="img-responsive">
</div>


## 10 pts  MipMaping
**Related source file: **
<ul>
    <li>include/nori/ray.h</li>
    <li>include/nori/shape.h</li>
    <li>src/mipmaptexture.cpp</li>
    <li>src/path_mis_mip.cpp</li>
    <li>src/mesh.cpp</li>
    <li>src/perspective.cpp</li>
</ul>

**mipmaptexture.cpp**
<br>
According to the book pbrtv3, mipMap should be intergrated in imageTexure, 
and the new class be able generate all level of mipmaps and provide interface to query color in different level mipmap. <br>
So I write a generateMipmap() function and will be call when a imageFile is loaded. <br>
It calculate the max level by log2(MaxResolusion), and create higher level of Mipmap with half of resolution compared to the lower level both in height and width, by averaging the 4 neighbor pixel color into one pixel to the next level. <br>
When being query, it take the differential value L to culculate the Mipmap level and perform trilinear interpolation to compute the color between level and level+1. <br>

<div class="image-container2">
    <img src="images/Level_1.jpg" alt="0">
    <img src="images/Level_2.jpg" alt="1">
    <img src="images/Level_3.jpg" alt="2">
    <img src="images/Level_4.jpg" alt="3">
    <img src="images/Level_5.jpg" alt="4">
    <img src="images/Level_6.jpg" alt="5">
    <img src="images/Level_7.jpg" alt="6">
    <img src="images/Level_8.jpg" alt="7">
    <img src="images/Level_9.jpg" alt="8">
    <img src="images/Level_10.jpg" alt="9">
</div>

**ray.h**
<br>
Add rayDifferential variable as pbrtv3 to ray Class.

**perspective.cpp**
<br>
Add genertateRayDifferential as pbrtv3 introduce. 
The RayDifferentials are generated by finding the Raydifferential in 1px offset in x and y direction of the camare film. 
Need to perform the transformation from clip coord to camara coord then to world coord.

**shape.h, mesh.cpp**
<br>
Add RayDifferentials related variables in Interscetion structure to record differentials along uv, xy, on hitting point p.<br>
When mesh being hit, compute the differential value(changing rate from clip coord to world coord) in setHitInfomation().<br>


**path_mis_mip.cpp**
<br>
Pass the differential value inside Interscetion into the mipMap query to get the color.<br>

<div class="twentytwenty-container">
    <img src="images/high_imagemapping.png" alt="Image_texture" class="img-responsive">
    <img src="images/high_mipmapping.png" alt="Mipmap_texture" class="img-responsive">
</div>
Unluckly I coudn't find the camera figure when I create the Level_distribution img... Thus I adjust for a while but still failed align them again..
<div class="twentytwenty-container">
    <img src="images/mipmapping.png" alt="Level_distribution" class="img-responsive">
    <img src="images/high_mipmapping.png" alt="Mipmap_texture" class="img-responsive">
</div>


## 15 pts Realistic camera model
**Related source file: **
<ul>
    <li>src/realistic.cpp</li>
    <li>src/toBeRealistic.cpp</li>
</ul>
I thought realistic is a super complex concept and is challenging. So I decide to implement advanced camera the thin lens model first.<br>
In realistic.cpp, we need to sample point on the 'lens', then use Gaussian lens equation to compute the focus point where sampled ray and central ray intersect, 
and finally to get the sampled ray from the sampled point on 'lens'.

<div class="twentytwenty-container">
    <img src="images/normalCamera.png" alt="normal_camera" class="img-responsive">
    <img src="images/small.png" alt="small_aperture" class="img-responsive">
    <img src="images/big.png" alt="Large_aperture" class="img-responsive">
    <img src="images/superbig.png" alt="exLarge_aperture" class="img-responsive">
</div>

However, due to the time limit, I haven't finish implementing the real realistic camare model.<br>
What I have got so far are the code imitation from pbrtv3, implementation of helper function and lens data loading in toBeRealistic.cpp.<br>


## 5 pts Intel Denoise

I faild to implement it because of the importing issues. I'v spent too much time on it but still get no result so i finally decided to give it up.

<div class="image-container2">
    <img src="images/denoise.png" alt="0">
</div>

# Final Image so far

<div class="image-container2">
    <img src="images/final.png" alt="0">
</div>
<!-- Bootstrap core CSS and JavaScript -->


<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.js"></script>

<script>
$(window).load(function(){$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5});});
</script>

<!-- Markdeep: -->
<script>var markdeepOptions = {onLoad: function() {$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5, move_slider_on_hover: true});},tocStyle:'none'};</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
